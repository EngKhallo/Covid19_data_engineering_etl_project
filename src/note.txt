- The project is about East african Covid-19 data from public api
- we are doing data engineering project in General:
    1. rest api public data 
    2. extract data and dump it to csv file
    3. then transform and some EDA on it
    4. Load it to a data warehouse: maybe snowflake or bigQuery
    5. do some aggregation on SQL 
    6. Airflow pipeline to automate data ingestion.
    7. Lastly, connect it to visualization app maybe Tableau.

1. Data Extractions

2. Data Transformation:
    - Handle missing values and duplicates.
    - Handling data types. 
    - Enrichment: Adding new metrics or columns/calculated columns.
    - Renaming columns for clarity.
    - check dates and everything else.

3. Data Loading:
    - Either I have two choices: 
        1. Load it to the cloud: Snowflake data warehouse or AWS Redshift.
        2. Load it to local database of PgSQL.
    - At first, I'll take the second option of loading the data to the local database

4. Analyze: build visualization using tableau

5. Automate: setup airflow to schedule etl process.

6. Document




